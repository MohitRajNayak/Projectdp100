{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install openpyxl"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting openpyxl\n  Downloading openpyxl-3.1.0-py2.py3-none-any.whl (250 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.0/250.0 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting et-xmlfile\n  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\nInstalling collected packages: et-xmlfile, openpyxl\nSuccessfully installed et-xmlfile-1.1.0 openpyxl-3.1.0\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1675342347611
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import azureml.core\r\n",
        "from azureml.core import Workspace\r\n",
        "\r\n",
        "ws = Workspace.from_config()\r\n",
        "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Ready to use Azure ML 1.48.0 to work with demo-dp100\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1675344880717
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a new folder for it\r\n",
        "import os, shutil\r\n",
        "\r\n",
        "# Create a folder for the experiment files\r\n",
        "folder_name = 'churn-experiment-files'\r\n",
        "experiment_folder = './' + folder_name\r\n",
        "os.makedirs(folder_name, exist_ok=True)\r\n",
        "\r\n",
        "# Copy the data file into the experiment folder\r\n",
        "shutil.copy('data/telco.xlsx', os.path.join(folder_name, \"telco.xlsx\"))"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "'churn-experiment-files/telco.xlsx'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1675344927847
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run the experiment as script**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $folder_name/churn_exp.py\r\n",
        "from azureml.core import Run\r\n",
        "import pandas as pd \r\n",
        "import os \r\n",
        "\r\n",
        "\r\n",
        "# Get the experiment run context\r\n",
        "run = Run.get_context()\r\n",
        "\r\n",
        "#Load the dataset\r\n",
        "data = pd.read_excel('telco.xlsx',engine='openpyxl')\r\n",
        "\r\n",
        "# Count the rows and log the result\r\n",
        "row_count = (len(data))\r\n",
        "run.log('observations', row_count)\r\n",
        "print('Analyzing {} rows of data'.format(row_count))\r\n",
        "\r\n",
        "# Count and log the label counts\r\n",
        "churn_counts = data['churn'].value_counts()\r\n",
        "print(churn_counts)\r\n",
        "for k, v in churn_counts.items():\r\n",
        "    run.log('Label:' + str(k), v)\r\n",
        "      \r\n",
        "# Save a sample of the data in the outputs folder (which gets uploaded automatically)\r\n",
        "os.makedirs('outputs', exist_ok=True)\r\n",
        "data.sample(100).to_csv(\"outputs/sample.csv\", index=False, header=True)\r\n",
        "\r\n",
        "# Complete the run\r\n",
        "run.complete()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting churn-experiment-files/churn_exp.py\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1675342710088
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Experiment, ScriptRunConfig, Environment\r\n",
        "from azureml.core.runconfig import DockerConfiguration\r\n",
        "from azureml.widgets import RunDetails\r\n",
        "\r\n",
        "# Create a Python environment for the experiment\r\n",
        "#env = Environment('demodp100-env')\r\n",
        "\r\n",
        "# Create a script config\r\n",
        "script_config = ScriptRunConfig(source_directory=experiment_folder,\r\n",
        "                                script='churn_exp.py',\r\n",
        "                                environment=experiment_env,\r\n",
        "                                docker_runtime_config=DockerConfiguration(use_docker=True))\r\n",
        "\r\n",
        "# submit the experiment\r\n",
        "experiment = Experiment(workspace=ws, name='mslearn-churn')\r\n",
        "run = experiment.submit(config=script_config)\r\n",
        "RunDetails(run).show()\r\n",
        "run.wait_for_completion()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a583acb1b1fa4128a0fd90cabefe2e40"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/aml.mini.widget.v1": "{\"status\": \"Starting\", \"workbench_run_details_uri\": \"https://ml.azure.com/runs/mslearn-churn_1675349534_d4d47d4e?wsid=/subscriptions/bd5c51d3-de0b-41fc-99f7-375e6737efda/resourcegroups/project_dp100/workspaces/demo-dp100&tid=474565c1-bca4-4295-a2f5-b0c7dbf2591c\", \"run_id\": \"mslearn-churn_1675349534_d4d47d4e\", \"run_properties\": {\"run_id\": \"mslearn-churn_1675349534_d4d47d4e\", \"created_utc\": \"2023-02-02T14:52:15.43834Z\", \"properties\": {\"_azureml.ComputeTargetType\": \"local\", \"ContentSnapshotId\": \"1f55004c-7c0a-4d91-8fc2-1e06f3a2c1f3\"}, \"tags\": {}, \"script_name\": null, \"arguments\": null, \"end_time_utc\": null, \"status\": \"Starting\", \"log_files\": {}, \"log_groups\": [], \"run_duration\": \"0:00:37\", \"run_number\": \"1675349535\", \"run_queued_details\": {\"status\": \"Starting\", \"details\": null}}, \"child_runs\": [], \"children_metrics\": {}, \"run_metrics\": [], \"run_logs\": \"Your job is submitted in Azure cloud and we are monitoring to get logs...\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.48.0\"}, \"loading\": false}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 18,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1675349521349
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $experiment_folder/experiment_env.yml\r\n",
        "name: experiment_env\r\n",
        "dependencies:\r\n",
        "  # The python interpreter version.\r\n",
        "  # Currently Azure ML only supports 3.5.2 and later.\r\n",
        "- python=3.6.2\r\n",
        "- scikit-learn\r\n",
        "- ipykernel\r\n",
        "- seaborn\r\n",
        "- scipy\r\n",
        "- numpy\r\n",
        "- matplotlib\r\n",
        "- pandas\r\n",
        "- pip\r\n",
        "- pip:\r\n",
        "  - azureml-defaults\r\n",
        "  - pyarrow"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Writing ./churn-experiment-files/experiment_env.yml\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Environment\r\n",
        "\r\n",
        "# Create a Python environment for the experiment (from a .yml file)\r\n",
        "experiment_env = Environment.from_conda_specification(\"experiment_env\", experiment_folder + \"/experiment_env.yml\")\r\n",
        "\r\n",
        "# Let Azure ML manage dependencies\r\n",
        "experiment_env.python.user_managed_dependencies = False \r\n",
        "\r\n",
        "# Print the environment details\r\n",
        "print(experiment_env.name, 'defined.')\r\n",
        "print(experiment_env.python.conda_dependencies.serialize_to_string())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "experiment_env defined.\nname: experiment_env\ndependencies:\n  # The python interpreter version.\n  # Currently Azure ML only supports 3.5.2 and later.\n- python=3.6.2\n- scikit-learn\n- ipykernel\n- seaborn\n- scipy\n- numpy\n- matplotlib\n- pandas\n- pip\n- pip:\n  - azureml-defaults\n  - pyarrow\n\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1675345067425
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the default datastore\r\n",
        "default_ds = ws.get_default_datastore()\r\n",
        "\r\n",
        "# Enumerate all datastores, indicating which is the default\r\n",
        "for ds_name in ws.datastores:\r\n",
        "    print(ds_name, \"- Default =\", ds_name == default_ds.name)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "workspacefilestore - Default = False\nworkspaceworkingdirectory - Default = False\nworkspaceartifactstore - Default = False\nworkspaceblobstore - Default = True\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1675346073426
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Dataset\r\n",
        "from azureml.data.datapath import DataPath\r\n",
        "\r\n",
        "Dataset.File.upload_directory(src_dir='data',\r\n",
        "                              target=DataPath(default_ds, 'chrun-data/')\r\n",
        "                              )"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Validating arguments.\nArguments validated.\nUploading file to chrun-data/\nUploading an estimated of 3 files\nUploading data/.amlignore\nUploaded data/.amlignore, 1 files out of an estimated total of 3\nUploading data/.amlignore.amltmp\nUploaded data/.amlignore.amltmp, 2 files out of an estimated total of 3\nUploading data/telco.xlsx\nUploaded data/telco.xlsx, 3 files out of an estimated total of 3\nUploaded 3 files\nCreating new dataset\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": "{\n  \"source\": [\n    \"('workspaceblobstore', '/chrun-data/')\"\n  ],\n  \"definition\": [\n    \"GetDatastoreFiles\"\n  ]\n}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1675346149750
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Dataset\r\n",
        "\r\n",
        "# Get the default datastore\r\n",
        "default_ds = ws.get_default_datastore()\r\n",
        "\r\n",
        "#Create a tabular dataset from the path on the datastore (this may take a short while)\r\n",
        "tab_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, 'chrun-data/telco.xlsx'))\r\n",
        "tab_data_set = Dataset.Tabular.f\r\n",
        "\r\n",
        "# Display the first 20 rows as a Pandas dataframe\r\n",
        "tab_data_set.take(20).to_pandas_dataframe()"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "DatasetValidationError",
          "evalue": "DatasetValidationError:\n\tMessage: Failed to validate the data.\nScriptExecutionException was caused by StreamAccessException.\n  StreamAccessException was caused by NotFoundException.\n    Found no resources for the input provided: '[REDACTED]'\n| session_id=b98b6169-aad9-47da-b752-664607f7c6f0\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Failed to validate the data.\\nScriptExecutionException was caused by StreamAccessException.\\n  StreamAccessException was caused by NotFoundException.\\n    Found no resources for the input provided: '[REDACTED]'\\n| session_id=b98b6169-aad9-47da-b752-664607f7c6f0\"\n    }\n}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mExecutionError\u001b[0m                            Traceback (most recent call last)",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/data/dataset_error_handling.py:65\u001b[0m, in \u001b[0;36m_validate_has_data\u001b[0;34m(dataflow, error_message)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[43mdataflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverify_has_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (dataprep()\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mdataflow\u001b[38;5;241m.\u001b[39mDataflowValidationError,\n\u001b[1;32m     67\u001b[0m         dataprep()\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39merrorhandlers\u001b[38;5;241m.\u001b[39mExecutionError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/dataprep/api/_loggerfactory.py:273\u001b[0m, in \u001b[0;36mtrack.<locals>.monitor.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/dataprep/api/dataflow.py:873\u001b[0m, in \u001b[0;36mDataflow.verify_has_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m EmptyStepsError()\n\u001b[0;32m--> 873\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_pyrecords\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    874\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DataflowValidationError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe Dataflow produced no records.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/dataprep/api/dataflow.py:779\u001b[0m, in \u001b[0;36mDataflow._to_pyrecords\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    778\u001b[0m         allow_fallback_to_clex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 779\u001b[0m intermediate_files \u001b[38;5;241m=\u001b[39m \u001b[43m_write_preppy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDataflow.to_pyrecords\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspan_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mto_dprep_span_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fallback_to_clex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_fallback_to_clex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_clex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_clex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(intermediate_files) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/dataprep/api/_dataframereader.py:300\u001b[0m, in \u001b[0;36m_write_preppy\u001b[0;34m(activity, dataflow, force_clex, allow_fallback_to_clex, span_context, telemetry_dict)\u001b[0m\n\u001b[1;32m    299\u001b[0m     cleanup()\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (intermediate_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_SUCCESS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mexists():\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/dataprep/api/_dataframereader.py:289\u001b[0m, in \u001b[0;36m_write_preppy\u001b[0;34m(activity, dataflow, force_clex, allow_fallback_to_clex, span_context, telemetry_dict)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 289\u001b[0m     \u001b[43m_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mactivity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataflow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_clex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_clex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspan_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspan_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_fallback_to_clex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_fallback_to_clex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcleanup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcleanup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtelemetry_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtelemetry_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/dataprep/api/_dataframereader.py:610\u001b[0m, in \u001b[0;36m_execute\u001b[0;34m(activity, dataflow, is_to_pandas_dataframe, force_clex, allow_fallback_to_clex, force_preppy, collect_results, fail_on_error, fail_on_mixed_types, fail_on_out_of_range_datetime, partition_ids, traceparent, span_context, cleanup, extended_types, nulls_as_nan, telemetry_dict, convert_preppy_to_pyrecords)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 610\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mclex_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _InconsistentSchemaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/dataprep/api/_dataframereader.py:439\u001b[0m, in \u001b[0;36m_execute.<locals>.clex_execute\u001b[0;34m()\u001b[0m\n\u001b[1;32m    438\u001b[0m activity_data \u001b[38;5;241m=\u001b[39m Dataflow\u001b[38;5;241m.\u001b[39m_dataflow_to_anonymous_activity_data(dataflow)\n\u001b[0;32m--> 439\u001b[0m \u001b[43mdataflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_anonymous_activity\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43mExecuteAnonymousActivityMessageArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43manonymous_activity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactivity_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspan_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspan_context\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_to_pandas_dataframe:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/dataprep/api/_aml_helper.py:44\u001b[0m, in \u001b[0;36mupdate_aml_env_vars.<locals>.decorator.<locals>.wrapper\u001b[0;34m(op_code, message, cancellation_token)\u001b[0m\n\u001b[1;32m     43\u001b[0m     engine_api_func()\u001b[38;5;241m.\u001b[39mupdate_environment_variable(changed)\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msend_message_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_token\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/dataprep/api/engineapi/api.py:159\u001b[0m, in \u001b[0;36mEngineAPI.execute_anonymous_activity\u001b[0;34m(self, message_args, cancellation_token)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;129m@update_aml_env_vars\u001b[39m(get_engine_api)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_anonymous_activity\u001b[39m(\u001b[38;5;28mself\u001b[39m, message_args: typedefinitions\u001b[38;5;241m.\u001b[39mExecuteAnonymousActivityMessageArguments, cancellation_token: CancellationToken \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_message_channel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_message\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEngine.ExecuteActivity\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/dataprep/api/engineapi/engine.py:291\u001b[0m, in \u001b[0;36mMultiThreadMessageChannel.send_message\u001b[0;34m(self, op_code, message, cancellation_token)\u001b[0m\n\u001b[1;32m    290\u001b[0m     cancel_on_error()\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mraise_engine_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43merror\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/dataprep/api/errorhandlers.py:10\u001b[0m, in \u001b[0;36mraise_engine_error\u001b[0;34m(error_response)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScriptExecution\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_code:\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ExecutionError(error_response)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_code:\n",
            "\u001b[0;31mExecutionError\u001b[0m: \nError Code: ScriptExecution.StreamAccess.NotFound\nFailed Step: 4b8ef095-42c5-4aae-b735-1e5e4429589b\nError Message: ScriptExecutionException was caused by StreamAccessException.\n  StreamAccessException was caused by NotFoundException.\n    Found no resources for the input provided: 'https://demodp1002328773093.blob.core.windows.net/azureml-blobstore-374986f6-1a08-4d30-928b-f715963df217/chrun-data/telco.csv'\n| session_id=b98b6169-aad9-47da-b752-664607f7c6f0",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mDatasetValidationError\u001b[0m                    Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[15], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m default_ds \u001b[38;5;241m=\u001b[39m ws\u001b[38;5;241m.\u001b[39mget_default_datastore()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#Create a tabular dataset from the path on the datastore (this may take a short while)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m tab_data_set \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTabular\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_delimited_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdefault_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchrun-data/telco.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Display the first 20 rows as a Pandas dataframe\u001b[39;00m\n\u001b[1;32m     10\u001b[0m tab_data_set\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m20\u001b[39m)\u001b[38;5;241m.\u001b[39mto_pandas_dataframe()\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/data/_loggerfactory.py:132\u001b[0m, in \u001b[0;36mtrack.<locals>.monitor.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _LoggerFactory\u001b[38;5;241m.\u001b[39mtrack_activity(logger, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, custom_dimensions) \u001b[38;5;28;01mas\u001b[39;00m al:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(al, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivity_info\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror_code\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/data/dataset_factory.py:366\u001b[0m, in \u001b[0;36mTabularDatasetFactory.from_delimited_files\u001b[0;34m(path, validate, include_path, infer_column_types, set_column_types, separator, header, partition_format, support_multi_line, empty_as_string, encoding)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    357\u001b[0m     dataflow \u001b[38;5;241m=\u001b[39m dataprep()\u001b[38;5;241m.\u001b[39mread_csv(path,\n\u001b[1;32m    358\u001b[0m                                    verify_exists\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    359\u001b[0m                                    include_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    363\u001b[0m                                    quoting\u001b[38;5;241m=\u001b[39msupport_multi_line,\n\u001b[1;32m    364\u001b[0m                                    encoding\u001b[38;5;241m=\u001b[39mencoding)\n\u001b[0;32m--> 366\u001b[0m dataflow \u001b[38;5;241m=\u001b[39m \u001b[43m_transform_and_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataflow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartition_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_column_types\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_is_inference_required\u001b[49m\u001b[43m(\u001b[49m\u001b[43mset_column_types\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m infer_column_types:\n\u001b[1;32m    371\u001b[0m     column_types_builder \u001b[38;5;241m=\u001b[39m dataflow\u001b[38;5;241m.\u001b[39mbuilders\u001b[38;5;241m.\u001b[39mset_column_types()\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/data/dataset_factory.py:1173\u001b[0m, in \u001b[0;36m_transform_and_validate\u001b[0;34m(dataflow, partition_format, include_path, validate, infer_column_types)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     dataflow \u001b[38;5;241m=\u001b[39m dataflow\u001b[38;5;241m.\u001b[39mdrop_columns(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPath\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate:\n\u001b[0;32m-> 1173\u001b[0m     \u001b[43m_validate_has_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataflow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFailed to validate the data.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m infer_column_types:\n\u001b[1;32m   1175\u001b[0m     _validate_has_data(dataflow, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to infer column type.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1176\u001b[0m                                  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mif data is inaccessible, please set infer_column_types to False.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/data/dataset_error_handling.py:68\u001b[0m, in \u001b[0;36m_validate_has_data\u001b[0;34m(dataflow, error_message)\u001b[0m\n\u001b[1;32m     65\u001b[0m     dataflow\u001b[38;5;241m.\u001b[39mverify_has_data()\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (dataprep()\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mdataflow\u001b[38;5;241m.\u001b[39mDataflowValidationError,\n\u001b[1;32m     67\u001b[0m         dataprep()\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39merrorhandlers\u001b[38;5;241m.\u001b[39mExecutionError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetValidationError(error_message \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m e\u001b[38;5;241m.\u001b[39mcompliant_message, exception\u001b[38;5;241m=\u001b[39me)\n",
            "\u001b[0;31mDatasetValidationError\u001b[0m: DatasetValidationError:\n\tMessage: Failed to validate the data.\nScriptExecutionException was caused by StreamAccessException.\n  StreamAccessException was caused by NotFoundException.\n    Found no resources for the input provided: '[REDACTED]'\n| session_id=b98b6169-aad9-47da-b752-664607f7c6f0\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Failed to validate the data.\\nScriptExecutionException was caused by StreamAccessException.\\n  StreamAccessException was caused by NotFoundException.\\n    Found no resources for the input provided: '[REDACTED]'\\n| session_id=b98b6169-aad9-47da-b752-664607f7c6f0\"\n    }\n}"
          ]
        }
      ],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1675346492035
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}